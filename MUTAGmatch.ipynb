{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# For reproducibility\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "def plotmutag(data):\n",
    "  cmap = colors.ListedColormap(['blue', 'black','red','yellow','orange','green','purple'])\n",
    "  ColorLegend = {'Carbon': 0,'Nitrogen': 1,'Oxygen': 2,'Fluorine': 3,'Iodine':4,'Chlorine':5,'Bromine':6}\n",
    "  cNorm  = colors.Normalize(vmin=0, vmax=6)\n",
    "  scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "  #print(cmap.colors)\n",
    "\n",
    "  exampledata=data\n",
    "  exfeatures=exampledata.x\n",
    "  #exlabel=exampledata.y\n",
    "  examplelabels=torch.argmax(exfeatures,dim=1)\n",
    "  #print(exlabel)\n",
    "  examplegraph=to_networkx(exampledata,to_undirected=True)\n",
    "  f = plt.figure(2,figsize=(8,8))\n",
    "  ax = f.add_subplot(1,1,1)\n",
    "  for label in ColorLegend:\n",
    "      ax.plot([0],[0],color=scalarMap.to_rgba(ColorLegend[label]),label=label)\n",
    "  nx.draw_networkx(examplegraph, node_size=150,node_color=examplelabels,cmap=cmap,vmin=0,vmax=6,with_labels=False,ax=ax)\n",
    "  plt.legend(fontsize=12,loc='best')\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test (e.g., 80% train, 20% test)\n",
    "num_train = int(0.8 * len(dataset))\n",
    "num_test = len(dataset) - num_train\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, inputdim, hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(inputdim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn = nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight=edge_weight)\n",
    "\n",
    "        node_embeddings = x  # Save node-level embeddings\n",
    "        graph_embedding = global_mean_pool(node_embeddings, batch)\n",
    "        graph_embedding = self.bn(graph_embedding)\n",
    "        graph_embedding = F.dropout(graph_embedding, p=0.5, training=self.training)\n",
    "        return graph_embedding, node_embeddings\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self, inputdim, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(inputdim, hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index,batch=None,edge_weight=None):\n",
    "        graph_embedding, node_embeddings = self.encoder(x, edge_index, batch, edge_weight)\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        return logits, node_embeddings  # <-- graph_embedding used internally, not returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_add_pool  # Changed from mean to add\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, inputdim, hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(inputdim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        x1 = self.leaky_relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x2 = self.leaky_relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x3 = self.conv3(x2, edge_index, edge_weight)\n",
    "\n",
    "        node_embeddings = x3\n",
    "        graph_embedding = global_add_pool(node_embeddings, batch)  # Changed from mean to add\n",
    "\n",
    "        return graph_embedding, node_embeddings\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.fc2 = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self, inputdim, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(inputdim, hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None, edge_weight=None):\n",
    "        graph_embedding, node_embeddings = self.encoder(x, edge_index, batch, edge_weight)\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        return logits, node_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_features=7\n",
    "inputdim=num_features\n",
    "model=CombinedModel(inputdim, hidden_channels=64,num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "# def train():\n",
    "#     model.train()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "#             embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#             #print(out)\n",
    "#             loss = criterion(out, data.y)  # Compute the loss.\n",
    "#             loss.backward()  # Derive gradients.\n",
    "#             optimizer.step()  # Update parameters based on gradients.\n",
    "#             optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "#         # Update the learning rate scheduler\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Print the current learning rate every epoch (optional)\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
    "#         # train_acc = test(train_loader)\n",
    "#         # test_acc = test(test_loader)\n",
    "#         # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            # Forward pass\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            out,embedding = model(x, edge_index, batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print the current learning rate and loss every epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}, Loss: {loss.item()}\")\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 700\n",
    "\n",
    "# Call the training loop\n",
    "train()\n",
    "\n",
    "\n",
    "# # Set the number of epochs\n",
    "# num_epochs = 800\n",
    "\n",
    "# # Call the training loop\n",
    "# train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, class_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
    "    and plot it with class names.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GNN model\n",
    "    - dataset: List of data objects\n",
    "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Evaluate the model and get predictions and true labels\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            out,_ = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Step 2: Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Step 3: Plot the confusion matrix\n",
    "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "\n",
    "# Example dataset (assuming it's a list of data objects)\n",
    "# dataset = [...]\n",
    "\n",
    "# Call the function with the model, dataset (as a list), and class dictionary\n",
    "#plot_confusion_matrix(model, dataset, class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0:'Non-Mutagenic',1:'Mutagenic'}\n",
    "plot_confusion_matrix(model,dataset,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mutag_classifier.pt')\n",
    "print(\"Model weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the model architecture manually\n",
    "# from your_module import GCNClassifierWithEmbeddings\n",
    "\n",
    "#model = GINClassifierWithEmbeddings(in_channels=1, hidden_channels=32, num_classes=2)\n",
    "model.load_state_dict(torch.load('mutag_classifier.pt'))\n",
    "#model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "\n",
    "print(\"Model weights loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.lap_solvers.hungarian import hungarian\n",
    "from src.lap_solvers.sinkhorn import Sinkhorn\n",
    "from itertools import product\n",
    "from src.spectral_clustering import spectral_clustering\n",
    "from src.utils.pad_tensor import pad_tensor\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start_time = 0\n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    def toc(self, str=\"\"):\n",
    "        print_helper('{:.5f}sec {}'.format(time.time()-self.start_time, str))\n",
    "\n",
    "DEBUG=False\n",
    "\n",
    "def print_helper(*args):\n",
    "    if DEBUG:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "class GA_GM(nn.Module):\n",
    "    \"\"\"\n",
    "    Graduated Assignment solver for\n",
    "     Graph Matching, Multi-Graph Matching and Multi-Graph Matching with a Mixture of Modes.\n",
    "\n",
    "    This operation does not support batched input, and all input tensors should not have the first batch dimension.\n",
    "\n",
    "    Parameter: maximum iteration mgm_iter\n",
    "               sinkhorn iteration sk_iter\n",
    "               initial sinkhorn regularization sk_tau0\n",
    "               sinkhorn regularization decaying factor sk_gamma\n",
    "               minimum tau value min_tau\n",
    "               convergence tolerance conv_tal\n",
    "    Input: multi-graph similarity matrix W\n",
    "           initial multi-matching matrix U0\n",
    "           number of nodes in each graph ms\n",
    "           size of universe n_univ\n",
    "           (optional) projector to doubly-stochastic matrix (sinkhorn) or permutation matrix (hungarian)\n",
    "    Output: multi-matching matrix U\n",
    "    \"\"\"\n",
    "    def __init__(self, mgm_iter=(200,), cluster_iter=10, sk_iter=20, sk_tau0=(0.5,), sk_gamma=0.5, cluster_beta=(1., 0.), converge_tol=1e-5, min_tau=(1e-2,), projector0=('sinkhorn',)):\n",
    "        super(GA_GM, self).__init__()\n",
    "        self.mgm_iter = mgm_iter\n",
    "        self.cluster_iter = cluster_iter\n",
    "        self.sk_iter = sk_iter\n",
    "        self.sk_tau0 = sk_tau0\n",
    "        self.sk_gamma = sk_gamma\n",
    "        self.cluster_beta = cluster_beta\n",
    "        self.converge_tol = converge_tol\n",
    "        self.min_tau = min_tau\n",
    "        self.projector0 = projector0\n",
    "\n",
    "    def forward(self, A, W, U0, ms, n_univ, quad_weight=1., cluster_quad_weight=1., num_clusters=2):\n",
    "        # gradient is not required for MGM module\n",
    "        W = W.detach()\n",
    "\n",
    "        num_graphs = ms.shape[0]\n",
    "        U = U0\n",
    "        m_indices = torch.cumsum(ms, dim=0)\n",
    "\n",
    "        Us = []\n",
    "        clusters = []\n",
    "\n",
    "        # initialize U with no clusters\n",
    "        cluster_M = torch.ones(num_graphs, num_graphs, device=A.device)\n",
    "        cluster_M01 = cluster_M\n",
    "\n",
    "        U = self.gagm(A, W, U, ms, n_univ, cluster_M, self.sk_tau0[0], self.min_tau[0], self.mgm_iter[0], self.projector0[0],\n",
    "                      quad_weight=quad_weight, hung_iter=(num_clusters == 1))\n",
    "        Us.append(U)\n",
    "\n",
    "        # MGM problem\n",
    "        if num_clusters == 1:\n",
    "            return U, torch.zeros(num_graphs, dtype=torch.int)\n",
    "\n",
    "        for beta, sk_tau0, min_tau, max_iter, projector0 in \\\n",
    "                zip(self.cluster_beta, self.sk_tau0, self.min_tau, self.mgm_iter, self.projector0):\n",
    "            for i in range(self.cluster_iter):\n",
    "                lastU = U\n",
    "\n",
    "                # clustering step\n",
    "                def get_alpha(scale=1., qw=1.):\n",
    "                    Alpha = torch.zeros(num_graphs, num_graphs, device=A.device)\n",
    "                    for idx1, idx2 in product(range(num_graphs), repeat=2):\n",
    "                        if idx1 == idx2:\n",
    "                            continue\n",
    "                        start_x = m_indices[idx1 - 1] if idx1 != 0 else 0\n",
    "                        end_x = m_indices[idx1]\n",
    "                        start_y = m_indices[idx2 - 1] if idx2 != 0 else 0\n",
    "                        end_y = m_indices[idx2]\n",
    "                        A_i = A[start_x:end_x, start_x:end_x]\n",
    "                        A_j = A[start_y:end_y, start_y:end_y]\n",
    "                        W_ij = W[start_x:end_x, start_y:end_y]\n",
    "                        U_i = U[start_x:end_x, :]\n",
    "                        U_j = U[start_y:end_y, :]\n",
    "                        X_ij = torch.mm(U_i, U_j.t())\n",
    "                        Alpha_ij = torch.sum(W_ij * X_ij) \\\n",
    "                                   + torch.exp(-torch.norm(torch.chain_matmul(X_ij.t(), A_i, X_ij) - A_j) / scale) * qw\n",
    "                        Alpha[idx1, idx2] = Alpha_ij\n",
    "                    return Alpha\n",
    "                Alpha = get_alpha(qw=cluster_quad_weight)\n",
    "\n",
    "                last_cluster_M01 = cluster_M01\n",
    "                cluster_v = spectral_clustering(Alpha, num_clusters, normalized=True)\n",
    "                cluster_M01 = (cluster_v.unsqueeze(0) == cluster_v.unsqueeze(1)).to(dtype=Alpha.dtype)\n",
    "                cluster_M = (1 - beta) * cluster_M01 + beta\n",
    "\n",
    "                if beta == self.cluster_beta[0] and i == 0:\n",
    "                    clusters.append(cluster_v)\n",
    "\n",
    "                # matching step\n",
    "                U = self.gagm(A, W, U, ms, n_univ, cluster_M, sk_tau0, min_tau, max_iter,\n",
    "                              projector='hungarian' if i != 0 else projector0, quad_weight=quad_weight,\n",
    "                              hung_iter=(beta == self.cluster_beta[-1]))\n",
    "\n",
    "                print_helper('beta = {:.2f}, delta U = {:.4f}, delta M = {:.4f}'.format(beta, torch.norm(lastU - U), torch.norm(last_cluster_M01 - cluster_M01)))\n",
    "\n",
    "                Us.append(U)\n",
    "                clusters.append(cluster_v)\n",
    "\n",
    "                if beta == 1:\n",
    "                    break\n",
    "\n",
    "                if torch.norm(lastU - U) < self.converge_tol and torch.norm(last_cluster_M01 - cluster_M01) < self.converge_tol:\n",
    "                    break\n",
    "\n",
    "        #return Us, clusters\n",
    "        return  U, cluster_v\n",
    "\n",
    "    def gagm(self, A, W, U0, ms, n_univ, cluster_M, init_tau, min_tau, max_iter, projector='sinkhorn', hung_iter=True, quad_weight=1.):\n",
    "        num_graphs = ms.shape[0]\n",
    "        U = U0\n",
    "        m_indices = torch.cumsum(ms, dim=0)\n",
    "\n",
    "        lastU = torch.zeros_like(U)\n",
    "\n",
    "        sinkhorn_tau = init_tau\n",
    "        #beta = 0.9\n",
    "        iter_flag = True\n",
    "\n",
    "        while iter_flag:\n",
    "            for i in range(max_iter):\n",
    "                lastU2 = lastU\n",
    "                lastU = U\n",
    "\n",
    "                # compact matrix form update of V\n",
    "                UUt = torch.mm(U, U.t())\n",
    "                cluster_weight = torch.repeat_interleave(cluster_M, ms.to(dtype=torch.long), dim=0)\n",
    "                cluster_weight = torch.repeat_interleave(cluster_weight, ms.to(dtype=torch.long), dim=1)\n",
    "                V = torch.chain_matmul(A, UUt * cluster_weight, A, U) * quad_weight * 2 + torch.mm(W * cluster_weight, U)\n",
    "                V /= num_graphs\n",
    "\n",
    "                U_list = []\n",
    "                if projector == 'hungarian':\n",
    "                    m_start = 0\n",
    "                    for m_end in m_indices:\n",
    "                        U_list.append(hungarian(V[m_start:m_end, :n_univ]))\n",
    "                        m_start = m_end\n",
    "                elif projector == 'sinkhorn':\n",
    "                    if torch.all(ms == ms[0]):\n",
    "                        if ms[0] <= n_univ:\n",
    "                            U_list.append(\n",
    "                                Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n",
    "                                    (V.reshape(num_graphs, -1, n_univ), dummy_row=True).reshape(-1, n_univ))\n",
    "                        else:\n",
    "                            U_list.append(\n",
    "                                Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n",
    "                                    (V.reshape(num_graphs, -1, n_univ).transpose(1, 2), dummy_row=True).transpose(1, 2).reshape(-1, n_univ))\n",
    "                    else:\n",
    "                        V_list = []\n",
    "                        n1 = []\n",
    "                        m_start = 0\n",
    "                        for m_end in m_indices:\n",
    "                            V_list.append(V[m_start:m_end, :n_univ])\n",
    "                            n1.append(m_end - m_start)\n",
    "                            m_start = m_end\n",
    "                        n1 = torch.tensor(n1)\n",
    "                        U = Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n",
    "                            (torch.stack(pad_tensor(V_list), dim=0), n1, dummy_row=True)\n",
    "                        m_start = 0\n",
    "                        for idx, m_end in enumerate(m_indices):\n",
    "                            U_list.append(U[idx, :m_end - m_start, :])\n",
    "                            m_start = m_end\n",
    "                else:\n",
    "                    raise NameError('Unknown projecter name: {}'.format(projector))\n",
    "\n",
    "                U = torch.cat(U_list, dim=0)\n",
    "                if num_graphs == 2:\n",
    "                    U[:ms[0], :] = torch.eye(ms[0], n_univ, device=U.device)\n",
    "\n",
    "                if torch.norm(U - lastU) < self.converge_tol or torch.norm(U - lastU2) == 0:\n",
    "                    break\n",
    "\n",
    "            if i == max_iter - 1: # not converged\n",
    "                if hung_iter:\n",
    "                    pass\n",
    "                else:\n",
    "                    U_list = [hungarian(_) for _ in U_list]\n",
    "                    U = torch.cat(U_list, dim=0)\n",
    "                    print_helper(i, 'max iter')\n",
    "                    break\n",
    "\n",
    "            # projection control\n",
    "            if projector == 'hungarian':\n",
    "                print_helper(i, 'hungarian')\n",
    "                break\n",
    "            elif sinkhorn_tau > min_tau:\n",
    "                print_helper(i, sinkhorn_tau)\n",
    "                sinkhorn_tau *= self.sk_gamma\n",
    "            else:\n",
    "                print_helper(i, sinkhorn_tau)\n",
    "                if hung_iter:\n",
    "                    projector = 'hungarian'\n",
    "                else:\n",
    "                    U_list = [hungarian(_) for _ in U_list]\n",
    "                    U = torch.cat(U_list, dim=0)\n",
    "                    break\n",
    "\n",
    "        return U\n",
    "\n",
    "\n",
    "class HiPPI(nn.Module):\n",
    "    \"\"\"\n",
    "    HiPPI solver for multiple graph matching: Higher-order Projected Power Iteration in ICCV 2019\n",
    "\n",
    "    This operation does not support batched input, and all input tensors should not have the first batch dimension.\n",
    "\n",
    "    Parameter: maximum iteration mgm_iter\n",
    "               sinkhorn iteration sk_iter\n",
    "               sinkhorn regularization sk_tau\n",
    "    Input: multi-graph similarity matrix W\n",
    "           initial multi-matching matrix U0\n",
    "           number of nodes in each graph ms\n",
    "           size of universe d\n",
    "           (optional) projector to doubly-stochastic matrix (sinkhorn) or permutation matrix (hungarian)\n",
    "    Output: multi-matching matrix U\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=50, sk_iter=20, sk_tau=1/200.):\n",
    "        super(HiPPI, self).__init__()\n",
    "        self.max_iter = max_iter\n",
    "        self.sinkhorn = Sinkhorn(max_iter=sk_iter, tau=sk_tau)\n",
    "        self.hungarian = hungarian\n",
    "\n",
    "    def forward(self, W, U0, ms, d, projector='sinkhorn'):\n",
    "        num_graphs = ms.shape[0]\n",
    "\n",
    "        U = U0\n",
    "        for i in range(self.max_iter):\n",
    "            lastU = U\n",
    "            WU = torch.mm(W, U) #/ num_graphs\n",
    "            V = torch.chain_matmul(WU, U.t(), WU) #/ num_graphs ** 2\n",
    "\n",
    "            #V_median = torch.median(torch.flatten(V, start_dim=-2), dim=-1).values\n",
    "            #V_var, V_mean = torch.var_mean(torch.flatten(V, start_dim=-2), dim=-1)\n",
    "            #V = V - V_mean\n",
    "            #V = V / torch.sqrt(V_var)\n",
    "\n",
    "            #V = V / V_median\n",
    "\n",
    "            U = []\n",
    "            m_start = 0\n",
    "            m_indices = torch.cumsum(ms, dim=0)\n",
    "            for m_end in m_indices:\n",
    "                if projector == 'sinkhorn':\n",
    "                    U.append(self.sinkhorn(V[m_start:m_end, :d], dummy_row=True))\n",
    "                elif projector == 'hungarian':\n",
    "                    U.append(self.hungarian(V[m_start:m_end, :d]))\n",
    "                else:\n",
    "                    raise NameError('Unknown projector {}.'.format(projector))\n",
    "                m_start = m_end\n",
    "            U = torch.cat(U, dim=0)\n",
    "\n",
    "            #print_helper('iter={}, diff={}, var={}, vmean={}, vvar={}'.format(i, torch.norm(U-lastU), torch.var(torch.sum(U, dim=0)), V_mean, V_var))\n",
    "\n",
    "            if torch.norm(U - lastU) < 1e-5:\n",
    "                print_helper(i)\n",
    "                break\n",
    "\n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Define target class (e.g., class 1)\n",
    "target_class = 0\n",
    "\n",
    "# Filter the dataset for graphs that are classified as the target class.\n",
    "selected_data = []\n",
    "for data in dataset:\n",
    "    # Run the classifier on each graph; model is assumed to be on CPU.\n",
    "    with torch.no_grad():\n",
    "        model = model.to('cpu')\n",
    "        out, _ = model(data.x,data.edge_index)\n",
    "        pred = out.argmax(dim=1).item()  # For a single graph, out is shape [1, num_classes]\n",
    "    if pred == target_class:\n",
    "        selected_data.append(data)\n",
    "\n",
    "print(\"Number of graphs classified as target class:\", len(selected_data))\n",
    "\n",
    "# --- Prepare global inputs for GA_GM on the selected graphs ---\n",
    "\n",
    "# 1. Compute node counts for each selected graph\n",
    "ms_sel = torch.tensor([data.num_nodes for data in selected_data], dtype=torch.long)\n",
    "\n",
    "# 2. Build a list of dense (binary) adjacency matrices for each selected graph\n",
    "adj_list_sel = []\n",
    "for data in selected_data:\n",
    "    A_dense = to_dense_adj(data.edge_index, max_num_nodes=data.num_nodes)[0]\n",
    "    A_dense = (A_dense > 0).float()  # Convert to binary adjacency\n",
    "    adj_list_sel.append(A_dense)\n",
    "\n",
    "# 3. Build a global block-diagonal adjacency matrix A_sel\n",
    "A_sel = torch.block_diag(*adj_list_sel)  # Shape: (total_nodes, total_nodes)\n",
    "\n",
    "# 4. Compute node embeddings for each selected graph using the trained GNN classifier.\n",
    "# Here we run the model to extract node embeddings.\n",
    "all_embeddings_sel = []\n",
    "for data in selected_data:\n",
    "    with torch.no_grad():\n",
    "        _, node_emb = model(data.x,data.edge_index,None)\n",
    "    all_embeddings_sel.append(node_emb)\n",
    "all_x_sel = torch.cat(all_embeddings_sel, dim=0)  # Shape: (total_nodes, hidden_channels)\n",
    "\n",
    "# 5. Compute the global node similarity matrix W_sel using the inner product of the node embeddings.\n",
    "W_sel = torch.mm(all_x_sel, all_x_sel.t())  # Shape: (total_nodes, total_nodes)\n",
    "\n",
    "# 6. Set universe size n_univ_sel. Here, we choose the maximum number of nodes among the selected graphs.\n",
    "n_univ_sel = 100#int(ms_sel.max().item())\n",
    "print(\"n_univ_sel:\", n_univ_sel)\n",
    "total_nodes_sel = int(ms_sel.sum().item())\n",
    "# Alternatively, you could use total_nodes_sel if you prefer a larger universe:\n",
    "# n_univ_sel = total_nodes_sel\n",
    "\n",
    "# 7. Initialize U0_sel: matching matrix of shape (total_nodes_sel, n_univ_sel)\n",
    "U0_sel = (1.0 / n_univ_sel) * torch.ones(total_nodes_sel, n_univ_sel) + 1e-3 * torch.randn(total_nodes_sel, n_univ_sel)\n",
    "\n",
    "print(\"Shape of all_x_sel\",all_x_sel.shape)\n",
    "print(\"Total nodes in selected graphs:\", total_nodes_sel)\n",
    "print(\"Global A_sel shape:\", A_sel.shape)\n",
    "print(\"Global W_sel shape:\", W_sel.shape)\n",
    "print(\"Initial U0_sel shape:\", U0_sel.shape)\n",
    "print(\"ms_sel:\", ms_sel)\n",
    "\n",
    "# --- Run the GA_GM solver on the selected graphs ---\n",
    "# For demonstration, we set the number of clusters to 2 (MGMC).\n",
    "num_clusters = 1\n",
    "\n",
    "# Instantiate the GA_GM solver (assumed to be already imported from src.lap_solvers)\n",
    "ga_gm_solver = GA_GM()  # Runs on CPU by default\n",
    "\n",
    "# Run the forward pass\n",
    "U_final_sel, clusters_sel = ga_gm_solver(\n",
    "    A_sel,          # Global block-diagonal adjacency matrix\n",
    "    W_sel,          # Global node similarity matrix computed from GNN embeddings\n",
    "    U0_sel,         # Initial matching matrix\n",
    "    ms_sel,         # Node counts per graph\n",
    "    n_univ_sel,     # Universe size (columns in U0_sel)\n",
    "    quad_weight=1.0,\n",
    "    cluster_quad_weight=1.0,\n",
    "    num_clusters=num_clusters  # >1 enables clustering\n",
    ")\n",
    "\n",
    "print(\"Final matching matrix U_final_sel shape:\", U_final_sel.shape)\n",
    "print(\"Final clustering vector (clusters_sel):\", clusters_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topk_graphs(model, dataset, target_class, k=5):\n",
    "    \"\"\"\n",
    "    Given a classifier and a target class, return the top-k graphs from the dataset\n",
    "    that the classifier assigns the highest confidence score for that class.\n",
    "\n",
    "    Returns:\n",
    "        topk_graphs: List of PyG Data objects\n",
    "        topk_indices: List of indices of top-k graphs in the original dataset\n",
    "        topk_scores: List of class scores assigned by the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    scores = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(data.x,data.edge_index,None)\n",
    "            prob = F.softmax(out, dim=1)[0, target_class].item()\n",
    "        scores.append((prob, data, i))\n",
    "\n",
    "    # Sort the list by score in descending order and pick top-k\n",
    "    topk = sorted(scores, key=lambda x: x[0], reverse=True)[:k]\n",
    "\n",
    "    # Unpack top-k results\n",
    "    topk_scores = [entry[0] for entry in topk]\n",
    "    topk_graphs = [entry[1] for entry in topk]\n",
    "    topk_indices = [entry[2] for entry in topk]\n",
    "\n",
    "    for rank, (idx, score) in enumerate(zip(topk_indices, topk_scores)):\n",
    "        print(f\"Graph Rank {rank+1}: Index = {idx}, Class Score = {score:.4f}\")\n",
    "\n",
    "    return topk_graphs, topk_indices, topk_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_graphs, topk_indices, topk_scores = find_topk_graphs(model, selected_data, target_class, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "class SharedGraphExplainer(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=32, temp_start=5.0, temp_end=0.1, epochs=300):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.temp_start = temp_start\n",
    "        self.temp_end = temp_end\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        logits = self.lin(h).squeeze(-1)  # Shape: (num_nodes,)\n",
    "        return logits\n",
    "\n",
    "    def sample_mask(self, logits):\n",
    "        temp = self.get_current_temp()\n",
    "        eps = 1e-20\n",
    "        uniform_noise = torch.rand_like(logits)\n",
    "        gumbel_noise = -torch.log(-torch.log(uniform_noise + eps) + eps)\n",
    "        y = logits + gumbel_noise\n",
    "        mask = torch.sigmoid(y / temp)\n",
    "        return mask\n",
    "\n",
    "    def get_current_temp(self):\n",
    "        return self.temp_start * (self.temp_end / self.temp_start) ** (self.current_epoch / self.epochs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def train_shared_explainer(model, topk_graphs, topk_indices, U_final_sel, ms_sel, n_univ_sel,\n",
    "                           target_class, topk_univ_k=5, epochs=300, lr=0.01,lambda_cls=1.0, lambda_align=1.0,\n",
    "                           lambda_sparsity=0.001, lambda_entropy=0.001,lambda_budget=1.0,budget=10):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device).eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False  # Freeze classifier\n",
    "\n",
    "    in_channels = topk_graphs[0].x.size(1)\n",
    "    explainer = SharedGraphExplainer(in_channels).to(device)\n",
    "    optimizer = torch.optim.Adam(explainer.parameters(), lr=lr)\n",
    "\n",
    "    graph_offsets = torch.cumsum(torch.cat([torch.tensor([0]), ms_sel]), dim=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        explainer.current_epoch=epoch\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(topk_graphs):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # === Node mask ===\n",
    "            logits = explainer(data.x, data.edge_index)\n",
    "            mask = explainer.sample_mask(logits)\n",
    "            print(f\"Mask value of graph {i} is\", mask)\n",
    "\n",
    "            # === Feature & edge masking ===\n",
    "            masked_x = data.x * mask.unsqueeze(-1)\n",
    "            src, dst = data.edge_index\n",
    "            edge_mask = mask[src] * mask[dst]  # soft edge weights\n",
    "\n",
    "            out, _ = model(masked_x, data.edge_index, data.batch, edge_weight=edge_mask)\n",
    "            class_score = out[0, target_class]\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            print(f\"Class Score achieved is for graph {i} is\", probs)\n",
    "            loss_cls = -class_score\n",
    "\n",
    "            # === Sparsity regularization ===\n",
    "            loss_sparsity = mask.mean()  # Encourage fewer nodes\n",
    "            # ==== Budget Regularization === #\n",
    "            loss_budget= F.relu(mask.sum() - budget)\n",
    "\n",
    "            # === Entropy regularization ===\n",
    "            mask_clipped = torch.clamp(mask, min=1e-6, max=1 - 1e-6)\n",
    "            loss_entropy = - (mask_clipped * torch.log(mask_clipped) + (1 - mask_clipped) * torch.log(1 - mask_clipped)).mean()\n",
    "\n",
    "            # === Alignment loss with transferred mask ===\n",
    "            start_i = graph_offsets[topk_indices[i]]\n",
    "            end_i = graph_offsets[topk_indices[i] + 1]\n",
    "            U_i = U_final_sel[start_i:end_i]  # shape: [n_i, n_univ]\n",
    "            univ_mask = torch.matmul(U_i.T, mask)  # shape: [n_univ]\n",
    "\n",
    "            loss_align = 0.0\n",
    "            for j in range(len(topk_graphs)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                start_j = graph_offsets[topk_indices[j]]\n",
    "                end_j = graph_offsets[topk_indices[j] + 1]\n",
    "                U_j = U_final_sel[start_j:end_j]  # shape: [n_j, n_univ]\n",
    "\n",
    "                # Transferred node mask to graph j\n",
    "                mask_j_transferred = torch.matmul(U_j, univ_mask)  # shape: [n_j]\n",
    "                data_j = topk_graphs[j].to(device)\n",
    "\n",
    "                masked_x_j = data_j.x * mask_j_transferred.unsqueeze(-1)\n",
    "                src_j, dst_j = data_j.edge_index\n",
    "                edge_mask_j = mask_j_transferred[src_j] * mask_j_transferred[dst_j]\n",
    "\n",
    "                out_j, _ = model(masked_x_j, data_j.edge_index, data_j.batch, edge_weight=edge_mask_j)\n",
    "                score_j = out_j[0, target_class]\n",
    "                loss_align -= score_j\n",
    "\n",
    "            # === Total loss ===\n",
    "            loss = lambda_cls*loss_cls + lambda_align * loss_align + lambda_sparsity * loss_sparsity + lambda_entropy * loss_entropy+lambda_budget * loss_budget\n",
    "            loss.backward()\n",
    "\n",
    "            #=== Diagnostics ===\n",
    "            # print(f\"[Epoch {epoch:03d}] Graph {i} | Mask mean: {mask.mean().item():.4f} | \"\n",
    "            #       f\"Class score: {class_score.item():.4f} | Loss_cls: {loss_cls.item():.4f} | \"\n",
    "            #       f\"Loss_sparsity: {loss_sparsity.item():.4f} | Loss_entropy: {loss_entropy.item():.4f}\")\n",
    "            # for name, param in explainer.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\" → {name}: grad norm = {param.grad.norm().item():.6f}\")\n",
    "            #     else:\n",
    "            #         print(f\" → {name}: ❌ NO GRADIENT\")\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"[Epoch {epoch:03d}] Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_class)\n",
    "print(topk_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = train_shared_explainer(\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    n_univ_sel=n_univ_sel,\n",
    "    target_class=target_class,\n",
    "    topk_univ_k=10,         # size of subgraph\n",
    "    epochs=130,\n",
    "    lr=0.001,\n",
    "    lambda_cls=2.5,\n",
    "    lambda_align=2.6,\n",
    "    lambda_entropy=0.2,\n",
    "    lambda_sparsity=4,\n",
    "    lambda_budget=10.0,\n",
    "    budget=8      # alignment weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "def evaluate_explanation_generalizability_thresholded(\n",
    "    explainer, model, topk_graphs, topk_indices, U_final_sel, ms_sel,\n",
    "    selected_data, selected_indices, score_filter_threshold=0.8, target_class=1,\n",
    "    save_path=\"explanation_generalization_results_filtered.pkl\"):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device).eval()\n",
    "    explainer = explainer.to(device).eval()\n",
    "\n",
    "    graph_offsets = torch.cumsum(torch.cat([torch.tensor([0]), ms_sel]), dim=0)\n",
    "    results = []\n",
    "\n",
    "    for i, ref_graph in enumerate(topk_graphs):\n",
    "        ref_graph = ref_graph.to(device)\n",
    "\n",
    "        # Get explanation mask from the reference graph\n",
    "        logits_ref = explainer(ref_graph.x, ref_graph.edge_index)\n",
    "        mask_ref = explainer.sample_mask(logits_ref).detach()\n",
    "\n",
    "        # Project to universal mask\n",
    "        start_i = graph_offsets[topk_indices[i]]\n",
    "        end_i = graph_offsets[topk_indices[i] + 1]\n",
    "        U_i = U_final_sel[start_i:end_i]\n",
    "        univ_mask = torch.matmul(U_i.T, mask_ref)\n",
    "\n",
    "        high_score_count = 0\n",
    "        filtered_total = 0\n",
    "        deltas = []\n",
    "        masked_scores = []\n",
    "        original_scores = []\n",
    "\n",
    "        for j, data_j in enumerate(selected_data):\n",
    "            data_j = data_j.to(device)\n",
    "\n",
    "            start_j = graph_offsets[selected_indices[j]]\n",
    "            end_j = graph_offsets[selected_indices[j] + 1]\n",
    "            U_j = U_final_sel[start_j:end_j]\n",
    "\n",
    "            # Original class score\n",
    "            with torch.no_grad():\n",
    "                out_orig, _ = model(data_j.x, data_j.edge_index, data_j.batch)\n",
    "                orig_score = F.softmax(out_orig, dim=1)[0, target_class].item()\n",
    "\n",
    "            if orig_score < score_filter_threshold:\n",
    "                continue  # Skip this graph if original confidence is too low\n",
    "\n",
    "            # Transfer mask\n",
    "            mask_j = torch.matmul(U_j, univ_mask)\n",
    "\n",
    "            # Masked class score\n",
    "            masked_x_j = data_j.x * mask_j.unsqueeze(-1)\n",
    "            src_j, dst_j = data_j.edge_index\n",
    "            edge_mask_j = mask_j[src_j] * mask_j[dst_j]\n",
    "\n",
    "            out_masked, _ = model(masked_x_j, data_j.edge_index, data_j.batch, edge_weight=edge_mask_j)\n",
    "            masked_score = F.softmax(out_masked, dim=1)[0, target_class].item()\n",
    "\n",
    "            delta = masked_score - orig_score\n",
    "            deltas.append(delta)\n",
    "            masked_scores.append(masked_score)\n",
    "            original_scores.append(orig_score)\n",
    "\n",
    "            if masked_score >= score_filter_threshold:\n",
    "                high_score_count += 1\n",
    "\n",
    "            filtered_total += 1\n",
    "\n",
    "        print(f\"Explanation {i}:\")\n",
    "        print(f\" → Filtered evaluation on {filtered_total} graphs with original score > {score_filter_threshold}\")\n",
    "        print(f\" → High masked score count (>{score_filter_threshold}): {high_score_count}/{filtered_total}\")\n",
    "\n",
    "        results.append({\n",
    "            'ref_index': i,\n",
    "            'high_score_count': high_score_count,\n",
    "            'high_score_ratio': high_score_count / filtered_total if filtered_total > 0 else 0.0,\n",
    "            'deltas': deltas,\n",
    "            'masked_scores': masked_scores,\n",
    "            'original_scores': original_scores,\n",
    "            'univ_mask': univ_mask.detach().cpu()\n",
    "        })\n",
    "\n",
    "    # Save all results\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    print(f\"\\n✅ Filtered generalization results saved to {save_path}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_explanation_generalizability_thresholded(\n",
    "    explainer=explainer,\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    selected_data=selected_data,\n",
    "    selected_indices=list(range(len(selected_data))),  # assuming sequential match\n",
    "    score_filter_threshold=0.8,\n",
    "    target_class=target_class,\n",
    "    save_path=\"generalization_results_filtered.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the results from the saved file if not already in memory\n",
    "import pickle\n",
    "with open(\"generalization_results_filtered.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Find the result with the highest high_score_count\n",
    "best_result = max(results, key=lambda x: x['high_score_count'])\n",
    "best_ref_index = best_result['ref_index']\n",
    "\n",
    "# Retrieve the corresponding data object\n",
    "best_graph = topk_graphs[best_ref_index]  # This is the graph that generated the most transferable explanation\n",
    "\n",
    "print(f\"Best explanation is from graph index: {best_ref_index}\")\n",
    "print(f\"High score count: {best_result['high_score_count']}\")\n",
    "\n",
    "# Now `best_graph` is your Data object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import subgraph\n",
    "import pickle\n",
    "\n",
    "# === Step 1: Load results if not already in memory ===\n",
    "with open(\"generalization_results_filtered.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# === Step 2: Find the graph index with best generalization ===\n",
    "best_result = max(results, key=lambda x: x['high_score_count'])\n",
    "best_ref_index = best_result['ref_index']\n",
    "print(f\"Best explanation from graph index {best_ref_index} with high score count = {best_result['high_score_count']}\")\n",
    "\n",
    "# === Step 3: Get the corresponding Data object ===\n",
    "best_graph = topk_graphs[best_ref_index].to('cpu')\n",
    "\n",
    "# === Step 4: Get the corresponding node mask ===\n",
    "with torch.no_grad():\n",
    "    logits = explainer(best_graph.x, best_graph.edge_index)\n",
    "    node_mask = explainer.sample_mask(logits).detach().cpu()\n",
    "\n",
    "# === Step 5: Threshold the mask to get node indices to keep ===\n",
    "threshold = 0.5  # You can change this\n",
    "selected_nodes = torch.where(node_mask > threshold)[0]\n",
    "print(f\"Number of selected nodes (mask > {threshold}): {len(selected_nodes)}\")\n",
    "\n",
    "# === Step 6: Extract the subgraph ===\n",
    "sub_edge_index, _ = subgraph(\n",
    "    subset=selected_nodes,\n",
    "    edge_index=best_graph.edge_index,\n",
    "    relabel_nodes=True\n",
    ")\n",
    "\n",
    "# === Step 7: Create new subgraph object (optional) ===\n",
    "from torch_geometric.data import Data\n",
    "subgraph_data = Data(\n",
    "    x=best_graph.x[selected_nodes],\n",
    "    edge_index=sub_edge_index\n",
    ")\n",
    "plotmutag(subgraph_data)\n",
    "\n",
    "# Now subgraph_data contains your masked subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import subgraph\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# === Step 1: Load results if not already in memory ===\n",
    "with open(\"generalization_results_filtered.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# === Step 2: Get top 3 graphs with highest high_score_count ===\n",
    "top_results = sorted(results, key=lambda x: x['high_score_count'], reverse=True)[:3]\n",
    "\n",
    "threshold = 0.5  # Change if needed\n",
    "\n",
    "for rank, result in enumerate(top_results, 1):\n",
    "    ref_index = result['ref_index']\n",
    "    print(f\"\\n[Rank {rank}] Graph index {ref_index} with high score count = {result['high_score_count']}\")\n",
    "\n",
    "    # === Get the corresponding Data object ===\n",
    "    graph = topk_graphs[ref_index].to('cpu')\n",
    "\n",
    "    # === Get the corresponding node mask ===\n",
    "    with torch.no_grad():\n",
    "        logits = explainer(graph.x, graph.edge_index)\n",
    "        node_mask = explainer.sample_mask(logits).detach().cpu()\n",
    "\n",
    "    # === Threshold the mask ===\n",
    "    selected_nodes = torch.where(node_mask > threshold)[0]\n",
    "    print(f\" → Number of selected nodes (mask > {threshold}): {len(selected_nodes)}\")\n",
    "\n",
    "    # === Extract the subgraph ===\n",
    "    sub_edge_index, _ = subgraph(\n",
    "        subset=selected_nodes,\n",
    "        edge_index=graph.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    # === Construct and plot subgraph ===\n",
    "    subgraph_data = Data(\n",
    "        x=graph.x[selected_nodes],\n",
    "        edge_index=sub_edge_index\n",
    "    )\n",
    "\n",
    "    print(f\" → Plotting subgraph for Rank {rank}\")\n",
    "    plotmutag(subgraph_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MUTAG dataset\n",
    "#ataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "\n",
    "# Placeholder: known halogen indices in MUTAG's one-hot encoding\n",
    "# These are dataset-specific; for MUTAG, the common atom types are:\n",
    "# C (carbon), N (nitrogen), O (oxygen), F (fluorine), I (iodine), Cl (chlorine), Br (bromine)\n",
    "# You may need to print dataset[0].x to identify indices exactly\n",
    "halogen_indices = [3, 4, 5, 6]  # Example: F, I, Cl, Br — adjust if needed!\n",
    "\n",
    "# Counters\n",
    "halogen_counts = {0: 0, 1: 0}  # class -> total halogen atoms\n",
    "compound_counts = {0: 0, 1: 0}  # class -> number of compounds\n",
    "\n",
    "# Iterate over graphs\n",
    "for data in dataset:\n",
    "    label = int(data.y.item())\n",
    "    compound_counts[label] += 1\n",
    "\n",
    "    # Count halogen atoms in this graph\n",
    "    halogen_atom_mask = data.x[:, halogen_indices].sum(dim=1) > 0\n",
    "    halogen_count = halogen_atom_mask.sum().item()\n",
    "    halogen_counts[label] += halogen_count\n",
    "\n",
    "# Compute average halogen atoms per compound\n",
    "avg_halogen_per_compound = {\n",
    "    cls: halogen_counts[cls] / compound_counts[cls] for cls in halogen_counts\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"Total compounds:\", compound_counts)\n",
    "print(\"Total halogen atoms:\", halogen_counts)\n",
    "print(\"Average halogen atoms per compound:\")\n",
    "for cls, avg in avg_halogen_per_compound.items():\n",
    "    label = \"Non-Mutagenic\" if cls == 0 else \"Mutagenic\"\n",
    "    print(f\"  {label}: {avg:.2f}\")\n",
    "\n",
    "# Optional: plot\n",
    "plt.bar([\"Non-Mutagenic\", \"Mutagenic\"],\n",
    "        [avg_halogen_per_compound[0], avg_halogen_per_compound[1]],\n",
    "        color=[\"green\", \"red\"])\n",
    "plt.ylabel(\"Avg. Halogen Atoms per Compound\")\n",
    "plt.title(\"Halogen Frequency in MUTAG Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = train_shared_explainer(\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    n_univ_sel=n_univ_sel,\n",
    "    target_class=target_class,\n",
    "    topk_univ_k=10,         # size of subgraph\n",
    "    epochs=130,\n",
    "    lr=0.001,\n",
    "    lambda_cls=1.0,\n",
    "    lambda_align=2.6,\n",
    "    lambda_entropy=0.2,\n",
    "    lambda_sparsity=4,\n",
    "    lambda_budget=8.0,\n",
    "    budget=8      # alignment weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === TRAINING AND EVALUATION ===\n",
    "\n",
    "\n",
    "\n",
    "# === Shared Training Args ===\n",
    "shared_kwargs = dict(\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    n_univ_sel=n_univ_sel,\n",
    "    target_class=target_class,\n",
    "    topk_univ_k=10,\n",
    "    epochs=130,\n",
    "    lr=0.001,\n",
    "    lambda_cls=1.0,\n",
    "    \n",
    "    lambda_entropy=0.2,\n",
    "    lambda_sparsity=4,\n",
    "    lambda_budget=10.0,\n",
    "    budget=8,\n",
    ")\n",
    "\n",
    "# === Train WITH generalization loss ===\n",
    "print(\"🔁 Training WITH generalization loss...\")\n",
    "explainer_with = train_shared_explainer(\n",
    "    **shared_kwargs,\n",
    "    lambda_align=2.6\n",
    ")\n",
    "\n",
    "results_with_path = \"explanation_generalization_results_with_gen.pkl\"\n",
    "results_with = evaluate_explanation_generalizability_thresholded(\n",
    "    explainer=explainer_with,\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    selected_data=selected_data,\n",
    "    selected_indices=list(range(len(selected_data))),\n",
    "    target_class=target_class,\n",
    "    score_filter_threshold=0.8,\n",
    "    save_path=results_with_path\n",
    ")\n",
    "\n",
    "# === Train WITHOUT generalization loss ===\n",
    "print(\"\\n🔁 Training WITHOUT generalization loss...\")\n",
    "explainer_without = train_shared_explainer(\n",
    "    **shared_kwargs,\n",
    "    lambda_align=0.0\n",
    ")\n",
    "\n",
    "results_without_path = \"explanation_generalization_results_without_gen.pkl\"\n",
    "results_without = evaluate_explanation_generalizability_thresholded(\n",
    "    explainer=explainer_without,\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    selected_data=selected_data,\n",
    "    selected_indices=list(range(len(selected_data))),\n",
    "    target_class=target_class,\n",
    "    score_filter_threshold=0.8,\n",
    "    save_path=results_without_path\n",
    ")\n",
    "\n",
    "# === PLOTTING ===\n",
    "\n",
    "def extract_generalization_scores(results):\n",
    "    return [r['high_score_ratio'] for r in results]\n",
    "\n",
    "with_scores = extract_generalization_scores(results_with)\n",
    "without_scores = extract_generalization_scores(results_without)\n",
    "indices = list(range(len(with_scores)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(indices, with_scores, marker='o', label='With Generalization Loss')\n",
    "plt.plot(indices, without_scores, marker='x', label='Without Generalization Loss')\n",
    "plt.xlabel(\"Explanation Index\")\n",
    "plt.ylabel(\"Generalization Score\")\n",
    "plt.title(\"Comparison of Generalization Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generalization_score_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# === SUMMARY ===\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Mean Generalization Score (With):     {np.mean(with_scores):.4f}\")\n",
    "print(f\"Mean Generalization Score (Without):  {np.mean(without_scores):.4f}\")\n",
    "print(\"📊 Plot saved as: generalization_score_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOTTING: Grouped Bar Plot ===\n",
    "\n",
    "def extract_generalization_scores(results):\n",
    "    return [r['high_score_ratio'] for r in results]\n",
    "\n",
    "with_scores = extract_generalization_scores(results_with)\n",
    "without_scores = extract_generalization_scores(results_without)\n",
    "indices = list(range(len(with_scores)))\n",
    "\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(indices))  # Explanation indices\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x - bar_width/2, with_scores, bar_width, label='With Generalization Loss')\n",
    "plt.bar(x + bar_width/2, without_scores, bar_width, label='Without Generalization Loss')\n",
    "plt.xlabel(\"Explanation Index\")\n",
    "plt.ylabel(\"Generalization Score\")\n",
    "plt.title(\"Generalization Score per Explanation\")\n",
    "plt.xticks(x, [f\"{i}\" for i in indices])\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generalization_score_comparison_barplot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# === SUMMARY ===\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Mean Generalization Score (With):     {np.mean(with_scores):.4f}\")\n",
    "print(f\"Mean Generalization Score (Without):  {np.mean(without_scores):.4f}\")\n",
    "print(\"📊 Bar plot saved as: generalization_score_comparison_barplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Replace these with your actual result file paths ===\n",
    "with_gen_path = \"explanation_generalization_results_with_gen.pkl\"\n",
    "without_gen_path = \"explanation_generalization_results_without_gen.pkl\"\n",
    "\n",
    "# === Load generalization scores from saved result files ===\n",
    "def load_scores(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "    scores = [r[\"high_score_ratio\"] for r in results]\n",
    "    return scores\n",
    "\n",
    "# Load both sets of scores\n",
    "with_scores = load_scores(with_gen_path)\n",
    "without_scores = load_scores(without_gen_path)\n",
    "\n",
    "# === Plot Violin ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "data = [with_scores, without_scores]\n",
    "\n",
    "parts = plt.violinplot(data, showmeans=True, showextrema=True, showmedians=False)\n",
    "\n",
    "# Customize violin appearance\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "for i, pc in enumerate(parts['bodies']):\n",
    "    pc.set_facecolor(colors[i])\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "# Mean markers\n",
    "means = [np.mean(with_scores), np.mean(without_scores)]\n",
    "plt.scatter([1, 2], means, color='black', marker='o', label='Mean')\n",
    "\n",
    "# Axis labels and styling\n",
    "plt.xticks([1, 2], ['With Gen Loss', 'Without Gen Loss'])\n",
    "plt.ylabel(\"Generalization Score\")\n",
    "plt.title(\"Distribution of Generalization Scores Across Explanations for Mutagenic Class\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(\"violin_generalization_score.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# === Configurable grid of lambda values ===\n",
    "sparsity_range = [0, 2, 4, 6, 8]\n",
    "entropy_range = [0.0, 0.1, 0.2, 0.4, 0.6]\n",
    "\n",
    "# === Store mean class scores ===\n",
    "scores_grid = np.zeros((len(sparsity_range), len(entropy_range)))\n",
    "\n",
    "# === Run experiments ===\n",
    "for i, lambda_sparsity in enumerate(sparsity_range):\n",
    "    for j, lambda_entropy in enumerate(entropy_range):\n",
    "        print(f\"Training with λ_sparsity={lambda_sparsity}, λ_entropy={lambda_entropy}\")\n",
    "\n",
    "        explainer = train_shared_explainer(\n",
    "            model=model,\n",
    "            topk_graphs=topk_graphs,\n",
    "            topk_indices=topk_indices,\n",
    "            U_final_sel=U_final_sel,\n",
    "            ms_sel=ms_sel,\n",
    "            n_univ_sel=n_univ_sel,\n",
    "            target_class=target_class,\n",
    "            topk_univ_k=10,\n",
    "            epochs=130,\n",
    "            lr=0.001,\n",
    "            lambda_cls=2.0,\n",
    "            lambda_align=2.6,\n",
    "            lambda_entropy=lambda_entropy,\n",
    "            lambda_sparsity=lambda_sparsity,\n",
    "            lambda_budget=8.0,\n",
    "            budget=8\n",
    "        )\n",
    "\n",
    "        # Evaluate: average target class score on the reference topk graphs\n",
    "        explainer.eval()\n",
    "        total_score = 0.0\n",
    "        for g in topk_graphs:\n",
    "            g = g.to('cpu')\n",
    "            logits = explainer(g.x, g.edge_index)\n",
    "            mask = explainer.sample_mask(logits)\n",
    "            masked_x = g.x * mask.unsqueeze(-1)\n",
    "            src, dst = g.edge_index\n",
    "            edge_weight = mask[src] * mask[dst]\n",
    "\n",
    "            out, _ = model(masked_x, g.edge_index, g.batch, edge_weight=edge_weight)\n",
    "            score = out.softmax(dim=1)[0, target_class].item()\n",
    "            total_score += score\n",
    "\n",
    "        avg_score = total_score / len(topk_graphs)\n",
    "        scores_grid[i, j] = avg_score\n",
    "\n",
    "# === 3D Surface Plot ===\n",
    "X, Y = np.meshgrid(entropy_range, sparsity_range)\n",
    "Z = scores_grid\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel(\"λ_entropy\")\n",
    "ax.set_ylabel(\"λ_sparsity\")\n",
    "ax.set_zlabel(\"Avg Target Class Score\")\n",
    "ax.set_title(\"Effect of Sparsity and Entropy on Target Class Score on the Mutagenic Class\")\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"3d_ablation_entropy_sparsity_target_score.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3D Surface Plot ===\n",
    "X, Y = np.meshgrid(entropy_range, sparsity_range)\n",
    "Z = scores_grid\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel(\"λ_entropy\")\n",
    "ax.set_ylabel(\"λ_sparsity\")\n",
    "ax.set_zlabel(\"Avg Target Class Score\")\n",
    "ax.set_title(\"Effect of Sparsity and Entropy on Target Class Score on the Non-Mutagenic Class\")\n",
    "\n",
    "# Set Z axis limits (optional, adjust to your score range)\n",
    "ax.set_zlim(0, 1.0)  # This flattens the surface vertically\n",
    "\n",
    "# === Set background panes to white ===\n",
    "ax.xaxis.pane.set_facecolor((1.0, 1.0, 1.0, 1.0))  # X pane white\n",
    "ax.yaxis.pane.set_facecolor((1.0, 1.0, 1.0, 1.0))  # Y pane white\n",
    "ax.zaxis.pane.set_facecolor((1.0, 1.0, 1.0, 1.0))  # Z pane white\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"3d_ablation_entropy_sparsity_target_score.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(scores_grid, annot=True, fmt=\".3f\", cmap=\"rocket_r\", \n",
    "            xticklabels=entropy_range, yticklabels=sparsity_range, cbar_kws={\"label\": \"Target Class Score\"})\n",
    "plt.xlabel(\"λ_entropy\")\n",
    "plt.ylabel(\"λ_sparsity\")\n",
    "plt.title(\"Target Class Score (Mutagenic Class)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"heatmap_target_score_trendy.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert your score grid into a long-form DataFrame\n",
    "rows = []\n",
    "for i, lam_s in enumerate(sparsity_range):\n",
    "    for j, lam_e in enumerate(entropy_range):\n",
    "        rows.append({\n",
    "            'λ_sparsity': lam_s,\n",
    "            'λ_entropy': lam_e,\n",
    "            'Target Score': scores_grid[i, j]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joypy import joyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Sort values for smoother ridgeline\n",
    "df_sorted = df.sort_values(by=\"λ_sparsity\")\n",
    "\n",
    "# Joypy expects \"λ_sparsity\" as the category (by=...), and \"Target Score\" as the value\n",
    "plt.figure(figsize=(10, 6))\n",
    "joyplot(data=df_sorted, by=\"λ_sparsity\", column=\"Target Score\", \n",
    "        colormap=plt.cm.viridis, fade=True, linewidth=1)\n",
    "\n",
    "plt.title(\"Ridgeline Plot of Target Class Score Across λ_entropy (Grouped by λ_sparsity)\")\n",
    "plt.xlabel(\"Target Class Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ridgeline_target_score.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
