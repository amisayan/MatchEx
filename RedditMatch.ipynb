{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# For reproducibility\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "\n",
    "dataset = TUDataset(root=\"data/TUDataset\",name='REDDIT-BINARY')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "#data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "\n",
    "def prepare_dataset_x(dataset):\n",
    "    newdataset=[]\n",
    "    if dataset[0].x is None:\n",
    "        print(\"Oh Yeah!\")\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max( max_degree, degs[-1].max().item() )\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            print(\"Enter first if\")\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "                newdataset.append(data)\n",
    "        else:\n",
    "            print(\"Entered Else\")\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                #print(\"entered for\")\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "                newdataset.append(data)\n",
    "    return newdataset\n",
    "dataset=prepare_dataset_x(dataset)\n",
    "data=dataset[0]\n",
    "print()\n",
    "print(data.x.shape)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test (e.g., 80% train, 20% test)\n",
    "num_train = int(0.8 * len(dataset))\n",
    "num_test = len(dataset) - num_train\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, global_add_pool, JumpingKnowledge\n",
    "\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, inputdim, hidden_channels, num_layers=5):\n",
    "        super(GINEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            in_dim = inputdim if i == 0 else hidden_channels\n",
    "            nn_layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels)\n",
    "            )\n",
    "            self.convs.append(GINConv(nn_layer))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.jump = JumpingKnowledge(mode='cat')  # concatenate layer outputs\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = []\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            xs.append(x)\n",
    "\n",
    "        x = self.jump(xs)  # shape: [N, hidden * num_layers]\n",
    "        node_embeddings = x\n",
    "        graph_embedding = global_add_pool(x, batch)\n",
    "        return graph_embedding, node_embeddings\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, inputdim, hidden_channels, num_classes, num_layers=5):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GINEncoder(inputdim, hidden_channels, num_layers)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels * num_layers, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None, edge_weight=None):\n",
    "        graph_embedding, node_embeddings = self.encoder(x, edge_index, batch)\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        return logits, node_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_features=1\n",
    "inputdim=num_features\n",
    "model=CombinedModel(inputdim, hidden_channels=64,num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)  # Move batch to GPU\n",
    "\n",
    "            # Forward pass\n",
    "            out, embedding = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, LR: {scheduler.get_last_lr()[0]:.6f}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 700\n",
    "\n",
    "# Train the model\n",
    "train()\n",
    "\n",
    "# Move model back to CPU after training\n",
    "model = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "# def train():\n",
    "#     model.train()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "#             embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#             #print(out)\n",
    "#             loss = criterion(out, data.y)  # Compute the loss.\n",
    "#             loss.backward()  # Derive gradients.\n",
    "#             optimizer.step()  # Update parameters based on gradients.\n",
    "#             optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "#         # Update the learning rate scheduler\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Print the current learning rate every epoch (optional)\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
    "#         # train_acc = test(train_loader)\n",
    "#         # test_acc = test(test_loader)\n",
    "#         # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            # Forward pass\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            out,embedding = model(x, edge_index, batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print the current learning rate and loss every epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}, Loss: {loss.item()}\")\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 400\n",
    "\n",
    "# Call the training loop\n",
    "train()\n",
    "\n",
    "\n",
    "# # Set the number of epochs\n",
    "# num_epochs = 800\n",
    "\n",
    "# # Call the training loop\n",
    "# train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, class_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
    "    and plot it with class names.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GNN model\n",
    "    - dataset: List of data objects\n",
    "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Evaluate the model and get predictions and true labels\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            out,_ = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Step 2: Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Step 3: Plot the confusion matrix\n",
    "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Replace these with your actual result file paths ===\n",
    "with_gen_path = \"explanation_generalization_results_with_gen.pkl\"\n",
    "without_gen_path = \"explanation_generalization_results_without_gen.pkl\"\n",
    "\n",
    "# === Load generalization scores from saved result files ===\n",
    "def load_scores(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "    scores = [r[\"high_score_ratio\"] for r in results]\n",
    "    return scores\n",
    "\n",
    "# Load both sets of scores\n",
    "with_scores = load_scores(with_gen_path)\n",
    "without_scores = load_scores(without_gen_path)\n",
    "\n",
    "# === Plot Violin ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "data = [with_scores, without_scores]\n",
    "\n",
    "parts = plt.violinplot(data, showmeans=True, showextrema=True, showmedians=False)\n",
    "\n",
    "# Customize violin appearance\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "for i, pc in enumerate(parts['bodies']):\n",
    "    pc.set_facecolor(colors[i])\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "# Mean markers\n",
    "means = [np.mean(with_scores), np.mean(without_scores)]\n",
    "plt.scatter([1, 2], means, color='black', marker='o', label='Mean')\n",
    "\n",
    "# Axis labels and styling\n",
    "plt.xticks([1, 2], ['With Gen Loss', 'Without Gen Loss'])\n",
    "plt.ylabel(\"Generalization Score\")\n",
    "plt.title(\"Distribution of Generalization Scores Across Explanations for Mutagenic Class\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(\"violin_generalization_score.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Example dataset (assuming it's a list of data objects)\n",
    "# dataset = [...]\n",
    "\n",
    "# Call the function with the model, dataset (as a list), and class dictionary\n",
    "#plot_confusion_matrix(model, dataset, class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0:'Question/Answer',1:'Discussion'}\n",
    "plot_confusion_matrix(model,dataset,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'reddit_classifier.pt')\n",
    "print(\"Model weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the model architecture manually\n",
    "# from your_module import GCNClassifierWithEmbeddings\n",
    "\n",
    "#model = GINClassifierWithEmbeddings(in_channels=1, hidden_channels=32, num_classes=2)\n",
    "model.load_state_dict(torch.load('reddit_classifier.pt'))\n",
    "#model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "\n",
    "print(\"Model weights loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.lap_solvers.hungarian import hungarian\n",
    "from src.lap_solvers.sinkhorn import Sinkhorn\n",
    "from itertools import product\n",
    "from src.spectral_clustering import spectral_clustering\n",
    "from src.utils.pad_tensor import pad_tensor\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start_time = 0\n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    def toc(self, str=\"\"):\n",
    "        print_helper('{:.5f}sec {}'.format(time.time()-self.start_time, str))\n",
    "\n",
    "DEBUG=False\n",
    "\n",
    "def print_helper(*args):\n",
    "    if DEBUG:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "class GA_GM(nn.Module):\n",
    "    \"\"\"\n",
    "    Graduated Assignment solver for\n",
    "     Graph Matching, Multi-Graph Matching and Multi-Graph Matching with a Mixture of Modes.\n",
    "\n",
    "    This operation does not support batched input, and all input tensors should not have the first batch dimension.\n",
    "\n",
    "    Parameter: maximum iteration mgm_iter\n",
    "               sinkhorn iteration sk_iter\n",
    "               initial sinkhorn regularization sk_tau0\n",
    "               sinkhorn regularization decaying factor sk_gamma\n",
    "               minimum tau value min_tau\n",
    "               convergence tolerance conv_tal\n",
    "    Input: multi-graph similarity matrix W\n",
    "           initial multi-matching matrix U0\n",
    "           number of nodes in each graph ms\n",
    "           size of universe n_univ\n",
    "           (optional) projector to doubly-stochastic matrix (sinkhorn) or permutation matrix (hungarian)\n",
    "    Output: multi-matching matrix U\n",
    "    \"\"\"\n",
    "    def __init__(self, mgm_iter=(200,), cluster_iter=10, sk_iter=20, sk_tau0=(0.5,), sk_gamma=0.5, cluster_beta=(1., 0.), converge_tol=1e-5, min_tau=(1e-2,), projector0=('sinkhorn',)):\n",
    "        super(GA_GM, self).__init__()\n",
    "        self.mgm_iter = mgm_iter\n",
    "        self.cluster_iter = cluster_iter\n",
    "        self.sk_iter = sk_iter\n",
    "        self.sk_tau0 = sk_tau0\n",
    "        self.sk_gamma = sk_gamma\n",
    "        self.cluster_beta = cluster_beta\n",
    "        self.converge_tol = converge_tol\n",
    "        self.min_tau = min_tau\n",
    "        self.projector0 = projector0\n",
    "\n",
    "    def forward(self, A, W, U0, ms, n_univ, quad_weight=1., cluster_quad_weight=1., num_clusters=2):\n",
    "        # gradient is not required for MGM module\n",
    "        W = W.detach()\n",
    "\n",
    "        num_graphs = ms.shape[0]\n",
    "        U = U0\n",
    "        m_indices = torch.cumsum(ms, dim=0)\n",
    "\n",
    "        Us = []\n",
    "        clusters = []\n",
    "\n",
    "        # initialize U with no clusters\n",
    "        cluster_M = torch.ones(num_graphs, num_graphs, device=A.device)\n",
    "        cluster_M01 = cluster_M\n",
    "\n",
    "        U = self.gagm(A, W, U, ms, n_univ, cluster_M, self.sk_tau0[0], self.min_tau[0], self.mgm_iter[0], self.projector0[0],\n",
    "                      quad_weight=quad_weight, hung_iter=(num_clusters == 1))\n",
    "        Us.append(U)\n",
    "\n",
    "        # MGM problem\n",
    "        if num_clusters == 1:\n",
    "            return U, torch.zeros(num_graphs, dtype=torch.int)\n",
    "\n",
    "        for beta, sk_tau0, min_tau, max_iter, projector0 in \\\n",
    "                zip(self.cluster_beta, self.sk_tau0, self.min_tau, self.mgm_iter, self.projector0):\n",
    "            for i in range(self.cluster_iter):\n",
    "                lastU = U\n",
    "\n",
    "                # clustering step\n",
    "                def get_alpha(scale=1., qw=1.):\n",
    "                    Alpha = torch.zeros(num_graphs, num_graphs, device=A.device)\n",
    "                    for idx1, idx2 in product(range(num_graphs), repeat=2):\n",
    "                        if idx1 == idx2:\n",
    "                            continue\n",
    "                        start_x = m_indices[idx1 - 1] if idx1 != 0 else 0\n",
    "                        end_x = m_indices[idx1]\n",
    "                        start_y = m_indices[idx2 - 1] if idx2 != 0 else 0\n",
    "                        end_y = m_indices[idx2]\n",
    "                        A_i = A[start_x:end_x, start_x:end_x]\n",
    "                        A_j = A[start_y:end_y, start_y:end_y]\n",
    "                        W_ij = W[start_x:end_x, start_y:end_y]\n",
    "                        U_i = U[start_x:end_x, :]\n",
    "                        U_j = U[start_y:end_y, :]\n",
    "                        X_ij = torch.mm(U_i, U_j.t())\n",
    "                        Alpha_ij = torch.sum(W_ij * X_ij) \\\n",
    "                                   + torch.exp(-torch.norm(torch.chain_matmul(X_ij.t(), A_i, X_ij) - A_j) / scale) * qw\n",
    "                        Alpha[idx1, idx2] = Alpha_ij\n",
    "                    return Alpha\n",
    "                Alpha = get_alpha(qw=cluster_quad_weight)\n",
    "\n",
    "                last_cluster_M01 = cluster_M01\n",
    "                cluster_v = spectral_clustering(Alpha, num_clusters, normalized=True)\n",
    "                cluster_M01 = (cluster_v.unsqueeze(0) == cluster_v.unsqueeze(1)).to(dtype=Alpha.dtype)\n",
    "                cluster_M = (1 - beta) * cluster_M01 + beta\n",
    "\n",
    "                if beta == self.cluster_beta[0] and i == 0:\n",
    "                    clusters.append(cluster_v)\n",
    "\n",
    "                # matching step\n",
    "                U = self.gagm(A, W, U, ms, n_univ, cluster_M, sk_tau0, min_tau, max_iter,\n",
    "                              projector='hungarian' if i != 0 else projector0, quad_weight=quad_weight,\n",
    "                              hung_iter=(beta == self.cluster_beta[-1]))\n",
    "\n",
    "                print_helper('beta = {:.2f}, delta U = {:.4f}, delta M = {:.4f}'.format(beta, torch.norm(lastU - U), torch.norm(last_cluster_M01 - cluster_M01)))\n",
    "\n",
    "                Us.append(U)\n",
    "                clusters.append(cluster_v)\n",
    "\n",
    "                if beta == 1:\n",
    "                    break\n",
    "\n",
    "                if torch.norm(lastU - U) < self.converge_tol and torch.norm(last_cluster_M01 - cluster_M01) < self.converge_tol:\n",
    "                    break\n",
    "\n",
    "        #return Us, clusters\n",
    "        return  U, cluster_v\n",
    "\n",
    "    def gagm(self, A, W, U0, ms, n_univ, cluster_M, init_tau, min_tau, max_iter, projector='sinkhorn', hung_iter=True, quad_weight=1.):\n",
    "        num_graphs = ms.shape[0]\n",
    "        U = U0\n",
    "        m_indices = torch.cumsum(ms, dim=0)\n",
    "\n",
    "        lastU = torch.zeros_like(U)\n",
    "\n",
    "        sinkhorn_tau = init_tau\n",
    "        #beta = 0.9\n",
    "        iter_flag = True\n",
    "\n",
    "        while iter_flag:\n",
    "            for i in range(max_iter):\n",
    "                lastU2 = lastU\n",
    "                lastU = U\n",
    "\n",
    "                # compact matrix form update of V\n",
    "                UUt = torch.mm(U, U.t())\n",
    "                cluster_weight = torch.repeat_interleave(cluster_M, ms.to(dtype=torch.long), dim=0)\n",
    "                cluster_weight = torch.repeat_interleave(cluster_weight, ms.to(dtype=torch.long), dim=1)\n",
    "                V = torch.chain_matmul(A, UUt * cluster_weight, A, U) * quad_weight * 2 + torch.mm(W * cluster_weight, U)\n",
    "                V /= num_graphs\n",
    "\n",
    "                U_list = []\n",
    "                if projector == 'hungarian':\n",
    "                    m_start = 0\n",
    "                    for m_end in m_indices:\n",
    "                        U_list.append(hungarian(V[m_start:m_end, :n_univ]))\n",
    "                        m_start = m_end\n",
    "                elif projector == 'sinkhorn':\n",
    "                    if torch.all(ms == ms[0]):\n",
    "                        if ms[0] <= n_univ:\n",
    "                            U_list.append(\n",
    "                                Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n",
    "                                    (V.reshape(num_graphs, -1, n_univ), dummy_row=True).reshape(-1, n_univ))\n",
    "                        else:\n",
    "                            U_list.append(\n",
    "                                Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n",
    "                                    (V.reshape(num_graphs, -1, n_univ).transpose(1, 2), dummy_row=True).transpose(1, 2).reshape(-1, n_univ))\n",
    "                    else:\n",
    "                        V_list = []\n",
    "                        n1 = []\n",
    "                        m_start = 0\n",
    "                        for m_end in m_indices:\n",
    "                            V_list.append(V[m_start:m_end, :n_univ])\n",
    "                            n1.append(m_end - m_start)\n",
    "                            m_start = m_end\n",
    "                        n1 = torch.tensor(n1)\n",
    "                        U = Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n",
    "                            (torch.stack(pad_tensor(V_list), dim=0), n1, dummy_row=True)\n",
    "                        m_start = 0\n",
    "                        for idx, m_end in enumerate(m_indices):\n",
    "                            U_list.append(U[idx, :m_end - m_start, :])\n",
    "                            m_start = m_end\n",
    "                else:\n",
    "                    raise NameError('Unknown projecter name: {}'.format(projector))\n",
    "\n",
    "                U = torch.cat(U_list, dim=0)\n",
    "                if num_graphs == 2:\n",
    "                    U[:ms[0], :] = torch.eye(ms[0], n_univ, device=U.device)\n",
    "\n",
    "                if torch.norm(U - lastU) < self.converge_tol or torch.norm(U - lastU2) == 0:\n",
    "                    break\n",
    "\n",
    "            if i == max_iter - 1: # not converged\n",
    "                if hung_iter:\n",
    "                    pass\n",
    "                else:\n",
    "                    U_list = [hungarian(_) for _ in U_list]\n",
    "                    U = torch.cat(U_list, dim=0)\n",
    "                    print_helper(i, 'max iter')\n",
    "                    break\n",
    "\n",
    "            # projection control\n",
    "            if projector == 'hungarian':\n",
    "                print_helper(i, 'hungarian')\n",
    "                break\n",
    "            elif sinkhorn_tau > min_tau:\n",
    "                print_helper(i, sinkhorn_tau)\n",
    "                sinkhorn_tau *= self.sk_gamma\n",
    "            else:\n",
    "                print_helper(i, sinkhorn_tau)\n",
    "                if hung_iter:\n",
    "                    projector = 'hungarian'\n",
    "                else:\n",
    "                    U_list = [hungarian(_) for _ in U_list]\n",
    "                    U = torch.cat(U_list, dim=0)\n",
    "                    break\n",
    "\n",
    "        return U\n",
    "\n",
    "\n",
    "class HiPPI(nn.Module):\n",
    "    \"\"\"\n",
    "    HiPPI solver for multiple graph matching: Higher-order Projected Power Iteration in ICCV 2019\n",
    "\n",
    "    This operation does not support batched input, and all input tensors should not have the first batch dimension.\n",
    "\n",
    "    Parameter: maximum iteration mgm_iter\n",
    "               sinkhorn iteration sk_iter\n",
    "               sinkhorn regularization sk_tau\n",
    "    Input: multi-graph similarity matrix W\n",
    "           initial multi-matching matrix U0\n",
    "           number of nodes in each graph ms\n",
    "           size of universe d\n",
    "           (optional) projector to doubly-stochastic matrix (sinkhorn) or permutation matrix (hungarian)\n",
    "    Output: multi-matching matrix U\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=50, sk_iter=20, sk_tau=1/200.):\n",
    "        super(HiPPI, self).__init__()\n",
    "        self.max_iter = max_iter\n",
    "        self.sinkhorn = Sinkhorn(max_iter=sk_iter, tau=sk_tau)\n",
    "        self.hungarian = hungarian\n",
    "\n",
    "    def forward(self, W, U0, ms, d, projector='sinkhorn'):\n",
    "        num_graphs = ms.shape[0]\n",
    "\n",
    "        U = U0\n",
    "        for i in range(self.max_iter):\n",
    "            lastU = U\n",
    "            WU = torch.mm(W, U) #/ num_graphs\n",
    "            V = torch.chain_matmul(WU, U.t(), WU) #/ num_graphs ** 2\n",
    "\n",
    "            #V_median = torch.median(torch.flatten(V, start_dim=-2), dim=-1).values\n",
    "            #V_var, V_mean = torch.var_mean(torch.flatten(V, start_dim=-2), dim=-1)\n",
    "            #V = V - V_mean\n",
    "            #V = V / torch.sqrt(V_var)\n",
    "\n",
    "            #V = V / V_median\n",
    "\n",
    "            U = []\n",
    "            m_start = 0\n",
    "            m_indices = torch.cumsum(ms, dim=0)\n",
    "            for m_end in m_indices:\n",
    "                if projector == 'sinkhorn':\n",
    "                    U.append(self.sinkhorn(V[m_start:m_end, :d], dummy_row=True))\n",
    "                elif projector == 'hungarian':\n",
    "                    U.append(self.hungarian(V[m_start:m_end, :d]))\n",
    "                else:\n",
    "                    raise NameError('Unknown projector {}.'.format(projector))\n",
    "                m_start = m_end\n",
    "            U = torch.cat(U, dim=0)\n",
    "\n",
    "            #print_helper('iter={}, diff={}, var={}, vmean={}, vvar={}'.format(i, torch.norm(U-lastU), torch.var(torch.sum(U, dim=0)), V_mean, V_var))\n",
    "\n",
    "            if torch.norm(U - lastU) < 1e-5:\n",
    "                print_helper(i)\n",
    "                break\n",
    "\n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Define target class (e.g., class 1)\n",
    "target_class = 0\n",
    "\n",
    "# Filter the dataset for graphs that are classified as the target class.\n",
    "selected_data = []\n",
    "for data in dataset:\n",
    "    # Run the classifier on each graph; model is assumed to be on CPU.\n",
    "    with torch.no_grad():\n",
    "        model = model.to('cpu')\n",
    "        out, _ = model(data.x,data.edge_index)\n",
    "        pred = out.argmax(dim=1).item()  # For a single graph, out is shape [1, num_classes]\n",
    "    if pred == target_class:\n",
    "        selected_data.append(data)\n",
    "selected_data=selected_data[:20]\n",
    "\n",
    "print(\"Number of graphs classified as target class:\", len(selected_data))\n",
    "\n",
    "# --- Prepare global inputs for GA_GM on the selected graphs ---\n",
    "\n",
    "# 1. Compute node counts for each selected graph\n",
    "ms_sel = torch.tensor([data.num_nodes for data in selected_data], dtype=torch.long)\n",
    "\n",
    "# 2. Build a list of dense (binary) adjacency matrices for each selected graph\n",
    "adj_list_sel = []\n",
    "for data in selected_data:\n",
    "    A_dense = to_dense_adj(data.edge_index, max_num_nodes=data.num_nodes)[0]\n",
    "    A_dense = (A_dense > 0).float()  # Convert to binary adjacency\n",
    "    adj_list_sel.append(A_dense)\n",
    "\n",
    "# 3. Build a global block-diagonal adjacency matrix A_sel\n",
    "A_sel = torch.block_diag(*adj_list_sel)  # Shape: (total_nodes, total_nodes)\n",
    "\n",
    "# 4. Compute node embeddings for each selected graph using the trained GNN classifier.\n",
    "# Here we run the model to extract node embeddings.\n",
    "all_embeddings_sel = []\n",
    "for data in selected_data:\n",
    "    with torch.no_grad():\n",
    "        _, node_emb = model(data.x,data.edge_index,None)\n",
    "    all_embeddings_sel.append(node_emb)\n",
    "all_x_sel = torch.cat(all_embeddings_sel, dim=0)  # Shape: (total_nodes, hidden_channels)\n",
    "\n",
    "# 5. Compute the global node similarity matrix W_sel using the inner product of the node embeddings.\n",
    "W_sel = torch.mm(all_x_sel, all_x_sel.t())  # Shape: (total_nodes, total_nodes)\n",
    "\n",
    "# 6. Set universe size n_univ_sel. Here, we choose the maximum number of nodes among the selected graphs.\n",
    "n_univ_sel = 10#int(ms_sel.max().item())\n",
    "print(\"n_univ_sel:\", n_univ_sel)\n",
    "total_nodes_sel = int(ms_sel.sum().item())\n",
    "# Alternatively, you could use total_nodes_sel if you prefer a larger universe:\n",
    "# n_univ_sel = total_nodes_sel\n",
    "\n",
    "# 7. Initialize U0_sel: matching matrix of shape (total_nodes_sel, n_univ_sel)\n",
    "U0_sel = (1.0 / n_univ_sel) * torch.ones(total_nodes_sel, n_univ_sel) + 1e-3 * torch.randn(total_nodes_sel, n_univ_sel)\n",
    "\n",
    "print(\"Shape of all_x_sel\",all_x_sel.shape)\n",
    "print(\"Total nodes in selected graphs:\", total_nodes_sel)\n",
    "print(\"Global A_sel shape:\", A_sel.shape)\n",
    "print(\"Global W_sel shape:\", W_sel.shape)\n",
    "print(\"Initial U0_sel shape:\", U0_sel.shape)\n",
    "print(\"ms_sel:\", ms_sel)\n",
    "\n",
    "# --- Run the GA_GM solver on the selected graphs ---\n",
    "# For demonstration, we set the number of clusters to 2 (MGMC).\n",
    "num_clusters = 1\n",
    "\n",
    "# Instantiate the GA_GM solver (assumed to be already imported from src.lap_solvers)\n",
    "ga_gm_solver = GA_GM()  # Runs on CPU by default\n",
    "\n",
    "# Run the forward pass\n",
    "U_final_sel, clusters_sel = ga_gm_solver(\n",
    "    A_sel,          # Global block-diagonal adjacency matrix\n",
    "    W_sel,          # Global node similarity matrix computed from GNN embeddings\n",
    "    U0_sel,         # Initial matching matrix\n",
    "    ms_sel,         # Node counts per graph\n",
    "    n_univ_sel,     # Universe size (columns in U0_sel)\n",
    "    quad_weight=1.0,\n",
    "    cluster_quad_weight=1.0,\n",
    "    num_clusters=num_clusters  # >1 enables clustering\n",
    ")\n",
    "\n",
    "print(\"Final matching matrix U_final_sel shape:\", U_final_sel.shape)\n",
    "print(\"Final clustering vector (clusters_sel):\", clusters_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topk_graphs(model, dataset, target_class, k=5):\n",
    "    \"\"\"\n",
    "    Given a classifier and a target class, return the top-k graphs from the dataset\n",
    "    that the classifier assigns the highest confidence score for that class.\n",
    "\n",
    "    Returns:\n",
    "        topk_graphs: List of PyG Data objects\n",
    "        topk_indices: List of indices of top-k graphs in the original dataset\n",
    "        topk_scores: List of class scores assigned by the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    scores = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(data.x,data.edge_index,None)\n",
    "            prob = F.softmax(out, dim=1)[0, target_class].item()\n",
    "        scores.append((prob, data, i))\n",
    "\n",
    "    # Sort the list by score in descending order and pick top-k\n",
    "    topk = sorted(scores, key=lambda x: x[0], reverse=True)[:k]\n",
    "\n",
    "    # Unpack top-k results\n",
    "    topk_scores = [entry[0] for entry in topk]\n",
    "    topk_graphs = [entry[1] for entry in topk]\n",
    "    topk_indices = [entry[2] for entry in topk]\n",
    "\n",
    "    for rank, (idx, score) in enumerate(zip(topk_indices, topk_scores)):\n",
    "        print(f\"Graph Rank {rank+1}: Index = {idx}, Class Score = {score:.4f}\")\n",
    "\n",
    "    return topk_graphs, topk_indices, topk_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_graphs, topk_indices, topk_scores = find_topk_graphs(model, selected_data, target_class, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "class SharedGraphExplainer(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=32, temp_start=5.0, temp_end=0.1, epochs=300):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.temp_start = temp_start\n",
    "        self.temp_end = temp_end\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        logits = self.lin(h).squeeze(-1)  # Shape: (num_nodes,)\n",
    "        return logits\n",
    "\n",
    "    def sample_mask(self, logits):\n",
    "        temp = self.get_current_temp()\n",
    "        eps = 1e-20\n",
    "        uniform_noise = torch.rand_like(logits)\n",
    "        gumbel_noise = -torch.log(-torch.log(uniform_noise + eps) + eps)\n",
    "        y = logits + gumbel_noise\n",
    "        mask = torch.sigmoid(y / temp)\n",
    "        return mask\n",
    "\n",
    "    def get_current_temp(self):\n",
    "        return self.temp_start * (self.temp_end / self.temp_start) ** (self.current_epoch / self.epochs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def train_shared_explainer(model, topk_graphs, topk_indices, U_final_sel, ms_sel, n_univ_sel,\n",
    "                           target_class, topk_univ_k=5, epochs=300, lr=0.01,lambda_cls=1.0, lambda_align=1.0,\n",
    "                           lambda_sparsity=0.001, lambda_entropy=0.001,lambda_budget=1.0,budget=10):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device).eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False  # Freeze classifier\n",
    "\n",
    "    in_channels = topk_graphs[0].x.size(1)\n",
    "    explainer = SharedGraphExplainer(in_channels).to(device)\n",
    "    optimizer = torch.optim.Adam(explainer.parameters(), lr=lr)\n",
    "\n",
    "    graph_offsets = torch.cumsum(torch.cat([torch.tensor([0]), ms_sel]), dim=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        explainer.current_epoch=epoch\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(topk_graphs):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # === Node mask ===\n",
    "            logits = explainer(data.x, data.edge_index)\n",
    "            mask = explainer.sample_mask(logits)\n",
    "            print(f\"Mask value of graph {i} is\", mask)\n",
    "\n",
    "            # === Feature & edge masking ===\n",
    "            masked_x = data.x * mask.unsqueeze(-1)\n",
    "            src, dst = data.edge_index\n",
    "            edge_mask = mask[src] * mask[dst]  # soft edge weights\n",
    "\n",
    "            out, _ = model(masked_x, data.edge_index, data.batch, edge_weight=edge_mask)\n",
    "            class_score = out[0, target_class]\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            print(f\"Class Score achieved is for graph {i} is\", probs)\n",
    "            loss_cls = -class_score\n",
    "\n",
    "            # === Sparsity regularization ===\n",
    "            loss_sparsity = mask.mean()  # Encourage fewer nodes\n",
    "            # ==== Budget Regularization === #\n",
    "            loss_budget= F.relu(mask.sum() - budget)\n",
    "\n",
    "            # === Entropy regularization ===\n",
    "            mask_clipped = torch.clamp(mask, min=1e-6, max=1 - 1e-6)\n",
    "            loss_entropy = - (mask_clipped * torch.log(mask_clipped) + (1 - mask_clipped) * torch.log(1 - mask_clipped)).mean()\n",
    "\n",
    "            # === Alignment loss with transferred mask ===\n",
    "            start_i = graph_offsets[topk_indices[i]]\n",
    "            end_i = graph_offsets[topk_indices[i] + 1]\n",
    "            U_i = U_final_sel[start_i:end_i]  # shape: [n_i, n_univ]\n",
    "            univ_mask = torch.matmul(U_i.T, mask)  # shape: [n_univ]\n",
    "\n",
    "            loss_align = 0.0\n",
    "            for j in range(len(topk_graphs)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                start_j = graph_offsets[topk_indices[j]]\n",
    "                end_j = graph_offsets[topk_indices[j] + 1]\n",
    "                U_j = U_final_sel[start_j:end_j]  # shape: [n_j, n_univ]\n",
    "\n",
    "                # Transferred node mask to graph j\n",
    "                mask_j_transferred = torch.matmul(U_j, univ_mask)  # shape: [n_j]\n",
    "                data_j = topk_graphs[j].to(device)\n",
    "\n",
    "                masked_x_j = data_j.x * mask_j_transferred.unsqueeze(-1)\n",
    "                src_j, dst_j = data_j.edge_index\n",
    "                edge_mask_j = mask_j_transferred[src_j] * mask_j_transferred[dst_j]\n",
    "\n",
    "                out_j, _ = model(masked_x_j, data_j.edge_index, data_j.batch, edge_weight=edge_mask_j)\n",
    "                score_j = out_j[0, target_class]\n",
    "                loss_align -= score_j\n",
    "\n",
    "            # === Total loss ===\n",
    "            loss = lambda_cls*loss_cls + lambda_align * loss_align + lambda_sparsity * loss_sparsity + lambda_entropy * loss_entropy+lambda_budget * loss_budget\n",
    "            loss.backward()\n",
    "\n",
    "            #=== Diagnostics ===\n",
    "            # print(f\"[Epoch {epoch:03d}] Graph {i} | Mask mean: {mask.mean().item():.4f} | \"\n",
    "            #       f\"Class score: {class_score.item():.4f} | Loss_cls: {loss_cls.item():.4f} | \"\n",
    "            #       f\"Loss_sparsity: {loss_sparsity.item():.4f} | Loss_entropy: {loss_entropy.item():.4f}\")\n",
    "            # for name, param in explainer.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\" → {name}: grad norm = {param.grad.norm().item():.6f}\")\n",
    "            #     else:\n",
    "            #         print(f\" → {name}: ❌ NO GRADIENT\")\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"[Epoch {epoch:03d}] Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_class)\n",
    "print(topk_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = train_shared_explainer(\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    n_univ_sel=n_univ_sel,\n",
    "    target_class=target_class,\n",
    "    topk_univ_k=5,         # size of subgraph\n",
    "    epochs=100,\n",
    "    lr=0.001,\n",
    "    lambda_cls=20,\n",
    "    lambda_align=20,\n",
    "    lambda_entropy=0.01,\n",
    "    lambda_sparsity=0.1,\n",
    "    lambda_budget=0.0,\n",
    "    budget=17      # alignment weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "def evaluate_explanation_generalizability_thresholded(\n",
    "    explainer, model, topk_graphs, topk_indices, U_final_sel, ms_sel,\n",
    "    selected_data, selected_indices, score_filter_threshold=0.8, target_class=1,\n",
    "    save_path=\"explanation_generalization_results_filtered.pkl\"):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device).eval()\n",
    "    explainer = explainer.to(device).eval()\n",
    "\n",
    "    graph_offsets = torch.cumsum(torch.cat([torch.tensor([0]), ms_sel]), dim=0)\n",
    "    results = []\n",
    "\n",
    "    for i, ref_graph in enumerate(topk_graphs):\n",
    "        ref_graph = ref_graph.to(device)\n",
    "\n",
    "        # Get explanation mask from the reference graph\n",
    "        logits_ref = explainer(ref_graph.x, ref_graph.edge_index)\n",
    "        mask_ref = explainer.sample_mask(logits_ref).detach()\n",
    "\n",
    "        # Project to universal mask\n",
    "        start_i = graph_offsets[topk_indices[i]]\n",
    "        end_i = graph_offsets[topk_indices[i] + 1]\n",
    "        U_i = U_final_sel[start_i:end_i]\n",
    "        univ_mask = torch.matmul(U_i.T, mask_ref)\n",
    "\n",
    "        high_score_count = 0\n",
    "        filtered_total = 0\n",
    "        deltas = []\n",
    "        masked_scores = []\n",
    "        original_scores = []\n",
    "\n",
    "        for j, data_j in enumerate(selected_data):\n",
    "            data_j = data_j.to(device)\n",
    "\n",
    "            start_j = graph_offsets[selected_indices[j]]\n",
    "            end_j = graph_offsets[selected_indices[j] + 1]\n",
    "            U_j = U_final_sel[start_j:end_j]\n",
    "\n",
    "            # Original class score\n",
    "            with torch.no_grad():\n",
    "                out_orig, _ = model(data_j.x, data_j.edge_index, data_j.batch)\n",
    "                orig_score = F.softmax(out_orig, dim=1)[0, target_class].item()\n",
    "\n",
    "            if orig_score < score_filter_threshold:\n",
    "                continue  # Skip this graph if original confidence is too low\n",
    "\n",
    "            # Transfer mask\n",
    "            mask_j = torch.matmul(U_j, univ_mask)\n",
    "\n",
    "            # Masked class score\n",
    "            masked_x_j = data_j.x * mask_j.unsqueeze(-1)\n",
    "            src_j, dst_j = data_j.edge_index\n",
    "            edge_mask_j = mask_j[src_j] * mask_j[dst_j]\n",
    "\n",
    "            out_masked, _ = model(masked_x_j, data_j.edge_index, data_j.batch, edge_weight=edge_mask_j)\n",
    "            masked_score = F.softmax(out_masked, dim=1)[0, target_class].item()\n",
    "\n",
    "            delta = masked_score - orig_score\n",
    "            deltas.append(delta)\n",
    "            masked_scores.append(masked_score)\n",
    "            original_scores.append(orig_score)\n",
    "\n",
    "            if masked_score >= score_filter_threshold:\n",
    "                high_score_count += 1\n",
    "\n",
    "            filtered_total += 1\n",
    "\n",
    "        print(f\"Explanation {i}:\")\n",
    "        print(f\" → Filtered evaluation on {filtered_total} graphs with original score > {score_filter_threshold}\")\n",
    "        print(f\" → High masked score count (>{score_filter_threshold}): {high_score_count}/{filtered_total}\")\n",
    "\n",
    "        results.append({\n",
    "            'ref_index': i,\n",
    "            'high_score_count': high_score_count,\n",
    "            'high_score_ratio': high_score_count / filtered_total if filtered_total > 0 else 0.0,\n",
    "            'deltas': deltas,\n",
    "            'masked_scores': masked_scores,\n",
    "            'original_scores': original_scores,\n",
    "            'univ_mask': univ_mask.detach().cpu()\n",
    "        })\n",
    "\n",
    "    # Save all results\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    print(f\"\\n✅ Filtered generalization results saved to {save_path}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_explanation_generalizability_thresholded(\n",
    "    explainer=explainer,\n",
    "    model=model,\n",
    "    topk_graphs=topk_graphs,\n",
    "    topk_indices=topk_indices,\n",
    "    U_final_sel=U_final_sel,\n",
    "    ms_sel=ms_sel,\n",
    "    selected_data=selected_data,\n",
    "    selected_indices=list(range(len(selected_data))),  # assuming sequential match\n",
    "    score_filter_threshold=0.5,\n",
    "    target_class=target_class,\n",
    "    save_path=\"generalization_results_filtered.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Load generalization results\n",
    "with open(\"generalization_results_filtered.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Find the reference index with highest high_score_count\n",
    "best_entry = max(results, key=lambda x: x['masked_scores'])\n",
    "best_ref_index = best_entry['ref_index']\n",
    "print(f\"Best generalizing graph index: {best_ref_index} with count: {best_entry['high_score_count']}\")\n",
    "\n",
    "# Retrieve the mask of that graph\n",
    "ref_mask = best_entry['univ_mask']\n",
    "\n",
    "# Convert universal mask back to original graph node mask\n",
    "start = sum(ms_sel[:topk_indices[best_ref_index]])\n",
    "end = start + ms_sel[topk_indices[best_ref_index]]\n",
    "U_ref = U_final_sel[start:end]\n",
    "node_mask = torch.matmul(U_ref, ref_mask)  # shape: [n_nodes_ref]\n",
    "\n",
    "# Apply threshold to extract important nodes\n",
    "threshold = 0.8\n",
    "print(node_mask)\n",
    "important_nodes = (node_mask > threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Convert graph to networkx and extract subgraph\n",
    "ref_graph = topk_graphs[best_ref_index].to('cpu')\n",
    "G_nx = to_networkx(ref_graph, to_undirected=True)\n",
    "G_sub = G_nx.subgraph(important_nodes.tolist())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 6))\n",
    "pos = nx.spring_layout(G_sub, seed=42)\n",
    "node_colors = node_mask[important_nodes].numpy()\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G_sub,\n",
    "    pos,\n",
    "    node_color='blue',\n",
    "    \n",
    "    with_labels='False',\n",
    "    node_size=100,\n",
    "    edge_color=\"gray\"\n",
    ")\n",
    "#plt.title(f\"Best Explanation for Class {target_class}\")\n",
    "#plt.title(f\"Best Generalizing Explanation (Graph {best_ref_index})\\nMask Threshold > {threshold}\")\n",
    "#plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.Reds), label=\"Mask Value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"best_generalizing_subgraph.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Load generalization results\n",
    "with open(\"generalization_results_filtered.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Sort entries by high_score_count in descending order\n",
    "sorted_results = sorted(results, key=lambda x: x['high_score_count'], reverse=True)\n",
    "top_k = 5  # Plot top 3 graphs\n",
    "\n",
    "# Loop over top 3 graphs\n",
    "for rank, entry in enumerate(sorted_results[:top_k]):\n",
    "    ref_index = entry['ref_index']\n",
    "    ref_mask = entry['univ_mask']\n",
    "\n",
    "    # Convert universal mask back to original graph node mask\n",
    "    start = sum(ms_sel[:topk_indices[ref_index]])\n",
    "    end = start + ms_sel[topk_indices[ref_index]]\n",
    "    U_ref = U_final_sel[start:end]\n",
    "    node_mask = torch.matmul(U_ref, ref_mask)  # shape: [n_nodes_ref]\n",
    "\n",
    "    # Apply threshold to extract important nodes\n",
    "    threshold = 0.001\n",
    "    important_nodes = (node_mask > threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Convert graph to networkx and extract subgraph\n",
    "    ref_graph = topk_graphs[ref_index].to('cpu')\n",
    "    G_nx = to_networkx(ref_graph, to_undirected=True)\n",
    "    G_sub = G_nx.subgraph(important_nodes.tolist())\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    pos = nx.spring_layout(G_sub, seed=42)\n",
    "    nx.draw_networkx(\n",
    "        G_sub,\n",
    "        pos,\n",
    "        node_color='skyblue',\n",
    "        with_labels=False,\n",
    "        node_size=100,\n",
    "        edge_color=\"gray\"\n",
    "    )\n",
    "\n",
    "    #plt.title(f\"Top-{rank+1} Generalizing Explanation (Graph {ref_index})\\nHigh Score Count: {entry['high_score_count']}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"top{rank+1}_generalizing_subgraph.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import subgraph\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# === Step 1: Load results if not already in memory ===\n",
    "with open(\"generalization_results_filtered.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# === Step 2: Get top 3 graphs with highest high_score_count ===\n",
    "top_results = sorted(results, key=lambda x: x['high_score_count'], reverse=True)[:5]\n",
    "\n",
    "threshold = 0.5  # Change if needed\n",
    "\n",
    "for rank, result in enumerate(top_results, 1):\n",
    "    ref_index = result['ref_index']\n",
    "    print(f\"\\n[Rank {rank}] Graph index {ref_index} with high score count = {result['high_score_count']}\")\n",
    "\n",
    "    # === Get the corresponding Data object ===\n",
    "    graph = topk_graphs[ref_index].to('cpu')\n",
    "\n",
    "    # === Get the corresponding node mask ===\n",
    "    with torch.no_grad():\n",
    "        logits = explainer(graph.x, graph.edge_index)\n",
    "        node_mask = explainer.sample_mask(logits).detach().cpu()\n",
    "\n",
    "    # === Threshold the mask ===\n",
    "    selected_nodes = torch.where(node_mask > threshold)[0]\n",
    "    print(f\" → Number of selected nodes (mask > {threshold}): {len(selected_nodes)}\")\n",
    "\n",
    "    # === Extract the subgraph ===\n",
    "    sub_edge_index, _ = subgraph(\n",
    "        subset=selected_nodes,\n",
    "        edge_index=graph.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    # === Construct and plot subgraph ===\n",
    "    subgraph_data = Data(\n",
    "        x=graph.x[selected_nodes],\n",
    "        edge_index=sub_edge_index\n",
    "    )\n",
    "\n",
    "    print(f\" → Plotting subgraph for Rank {rank}\")\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    pos = nx.spring_layout(G_sub, seed=42)\n",
    "    #node_colors = node_mask[important_nodes].numpy()\n",
    "\n",
    "    nx.draw_networkx(\n",
    "        G_sub,\n",
    "        pos,\n",
    "        node_color='blue',\n",
    "        \n",
    "        with_labels='False',\n",
    "        node_size=100,\n",
    "        edge_color=\"gray\"\n",
    "    )\n",
    "    #plt.title(f\"Best Explanation for Class {target_class}\")\n",
    "    #plt.title(f\"Best Generalizing Explanation (Graph {best_ref_index})\\nMask Threshold > {threshold}\")\n",
    "    #plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.Reds), label=\"Mask Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"best_generalizing_subgraph.png\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
